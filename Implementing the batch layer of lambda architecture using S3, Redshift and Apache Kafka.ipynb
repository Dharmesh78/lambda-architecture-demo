{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the batch layer of lambda architecture using S3, Redshift and Apache Kafka\n",
    "\n",
    "### Purpose:\n",
    "- store all the tweets that were produced by Kafka Producer into S3\n",
    "- export them into Redshift\n",
    "- perform aggregation on the tweets to get the desired output of batch layer\n",
    "- achieve this by: \n",
    "    - every couple of hours get the latest unseen tweets produced by the Kafka Producer and store them into a S3 archive\n",
    "    - every night run a sql query to compute the result of batch layer\n",
    "\n",
    "### Contents: \n",
    "- [Defining the Kafka consumer](#1)\n",
    "- [Defining a Amazon Web Services S3 storage client](#2)\n",
    "- [Writing the data into a S3 bucket](#3)\n",
    "- [Exporting data from S3 bucket to Amazon Redshift using COPY command](#4)\n",
    "- [Aggregating \"raw\" tweets in Redshift](#5)\n",
    "- [Deployment](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "### Defining the Kafka consumer\n",
    "- setting the location of Kafka Broker\n",
    "- specifying the group_id and consumer_timeout\n",
    "- subsribing to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "                        bootstrap_servers='localhost:9092',\n",
    "                        auto_offset_reset='latest',  # Reset partition offsets upon OffsetOutOfRangeError\n",
    "                        group_id='test',   # must have a unique consumer group id \n",
    "                        consumer_timeout_ms=1000)  \n",
    "                                # How long to listen for messages - we do it for 10 seconds \n",
    "                                # because we poll the kafka broker only each couple of hours\n",
    "\n",
    "consumer.subscribe('tweets-lambda1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Defining a Amazon Web Services S3 storage client\n",
    "- setting the autohrizaition and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAIXUPHT6ERRMQYINQ',\n",
    "    aws_secret_access_key='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',\n",
    ")\n",
    "\n",
    "s3_client = s3_resource.meta.client\n",
    "bucket_name = 'lambda-architecture123'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "### Writing the data into a S3 bucket\n",
    "- polling the Kafka Broker\n",
    "- aggregating the latest messages into a single object in the bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_twitter_data(path):\n",
    "    csv_buffer = StringIO() # S3 storage is object storage -> our document is just a large string\n",
    "\n",
    "    for message in consumer: # this acts as \"get me an iterator over the latest messages I haven't seen\"\n",
    "        csv_buffer.write(message.value.decode() + '\\n') \n",
    "#        print(message)\n",
    "    s3_resource.Object(bucket_name,path).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Exporting data from S3 bucket to Amazon Redshift using COPY command\n",
    "- authenticate and create a connection using psycopg module\n",
    "- export data using COPY command from S3 to Redshift \"raw\" table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "config = { 'dbname': 'lambda', \n",
    "           'user':'dorian',\n",
    "           'pwd':'Demo1234',\n",
    "           'host':'data-warehouse.c3glymsgdgty.us-east-1.redshift.amazonaws.com',\n",
    "           'port':'5439'\n",
    "         }\n",
    "conn =  psycopg2.connect(dbname=config['dbname'], host=config['host'], \n",
    "                              port=config['port'], user=config['user'], \n",
    "                              password=config['pwd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(conn, path):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\" \n",
    "        copy \n",
    "            batch_raw\n",
    "        from \n",
    "            's3://lambda-architecture123/\"\"\" + path + \"\"\"'  \n",
    "            access_key_id 'AKIAIXUPHT6ERRMQYINQ'\n",
    "            secret_access_key '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'\n",
    "            delimiter ';'\n",
    "            region 'eu-central-1'\n",
    "    \"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the batch layer output\n",
    "- querying the raw tweets stored in redshift to get the desired batch layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_layer(conn):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\" \n",
    "        drop table if exists batch_layer;\n",
    "\n",
    "        with raw_dedup as (\n",
    "        SELECT\n",
    "            distinct id,created_at,followers_count,location,favorite_count,retweet_count\n",
    "        FROM\n",
    "            batch_raw\n",
    "        ),\n",
    "        batch_result as (\n",
    "            SELECT\n",
    "                location,\n",
    "                count(id) as count_id,\n",
    "                sum(followers_count) as sum_followers_count,\n",
    "                sum(favorite_count) as sum_favorite_count,\n",
    "                sum(retweet_count) as sum_retweet_count\n",
    "            FROM\n",
    "                raw_dedup\n",
    "            group by \n",
    "                location\n",
    "        )\n",
    "        select \n",
    "            *\n",
    "        INTO\n",
    "            batch_layer\n",
    "        FROM\n",
    "            batch_result\"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_batch_layer(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Deployment \n",
    "- perform the task every couple of hours and wait in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    while True:\n",
    "        path = 'tweets/'+ time.strftime(\"%Y/%m/%d/%H\") + '_tweets_' + str(random.randint(1,1000)) + '.log'\n",
    "        store_twitter_data(path)\n",
    "        copy_files(conn, path)\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodic_work(60 * 60) ## 60 minutes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tweets/'+ time.strftime(\"%Y/%m/%d/%H\") + '_tweets_' + str(random.randint(1,1000)) + '.log'\n",
    "\n",
    "store_twitter_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 4))\n",
      "\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "SSL SYSCALL error: Operation timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-283833030d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e3e962d1c071>\u001b[0m in \u001b[0;36mcopy_files\u001b[0;34m(conn, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdelimiter\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mregion\u001b[0m \u001b[0;34m'eu-central-1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \"\"\")\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: SSL SYSCALL error: Operation timed out\n"
     ]
    }
   ],
   "source": [
    "copy_files(conn, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run at the end of the day\n",
    "compute_batch_layer(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
